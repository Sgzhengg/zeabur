import os
import uuid
import shutil
import zipfile
import traceback
from pathlib import Path
from typing import List, Optional

# ğŸŸ¢ å¼•å…¥ Redis åº“
import redis

from fastapi import FastAPI, UploadFile, Form, HTTPException, File
from fastapi.middleware.cors import CORSMiddleware
from llama_parse import LlamaParse
from qdrant_client import QdrantClient, models
from flashrank import Ranker, RerankRequest
from pydantic import BaseModel

# ğŸ†• LlamaIndex ç›¸å…³å¯¼å…¥
try:
    from llama_index.core import Document
    from llama_index.core.node_parser import MarkdownElementNodeParser
    print("âœ… LlamaIndex modules imported successfully")
    HAS_LLAMAINDEX = True
except ImportError as e:
    print(f"âš ï¸ Warning: LlamaIndex import error: {e}")
    print("   Will use fallback mode (optimized chunking)")
    HAS_LLAMAINDEX = False
    MarkdownElementNodeParser = None
    Document = None

# --- 1. ç¯å¢ƒå˜é‡è¯»å– ---
LLAMA_CLOUD_API_KEY = os.getenv("LLAMA_CLOUD_API_KEY")
QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")

# ğŸŸ¢ Redis é…ç½®
REDIS_HOST = os.getenv("REDIS_HOST", "redis")
REDIS_PORT = int(os.getenv("REDIS_PORT", 6379))
REDIS_PASSWORD = os.getenv("REDIS_PASSWORD", None)

COLLECTION_NAME = "telecom_collection_v2"
TABLES_COLLECTION_NAME = "telecom_tables_v2"  # ğŸ†• ä¸“é—¨å­˜å‚¨è¡¨æ ¼

print(f"DEBUG CONFIG: QDRANT_URL={QDRANT_URL}, REDIS_HOST={REDIS_HOST}")

# --- 2. åˆå§‹åŒ– Re-ranker ---
print("â³ Initializing FlashRank Reranker...")
reranker = Ranker(model_name="ms-marco-MiniLM-L-12-v2", cache_dir="/tmp/flashrank_cache")
print("âœ… Reranker initialized!")

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

if not QDRANT_URL:
    raise ValueError("âŒ Fatal Error: QDRANT_URL is missing!")

# åˆå§‹åŒ– Qdrant
client = QdrantClient(
    url=QDRANT_URL,
    api_key=QDRANT_API_KEY,
    prefer_grpc=False
)

@app.on_event("startup")
def startup_event():
    print(f"ğŸš€ Connecting to Qdrant at: {QDRANT_URL} ...")
    try:
        collections = client.get_collections()
        print(f"âœ… Connected to Qdrant! Found {len(collections.collections)} collections.")
    except Exception as e:
        print(f"âŒ Qdrant Connection Failed! Error: {e}")

@app.get("/")
def health_check():
    return {
        "status": "ok",
        "service": "Telecom Ingest API (With MarkdownElementNodeParser)",
        "features": ["LlamaParse", "MarkdownElementNodeParser", "Table Extraction", "Qdrant+FlashRank"]
    }

# ========== Pydantic æ•°æ®æ¨¡å‹ ==========

class QueryAnalysisRequest(BaseModel):
    query: str

class ExtractTableRequest(BaseModel):
    document_id: str

class CompareDocumentsRequest(BaseModel):
    doc_ids: List[str]

# ========== è¾…åŠ©å‡½æ•° ==========

def extract_zip(zip_path: str, extract_to: str):
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_to)

def guess_doc_type(filename: str) -> str:
    main_keywords = ["é€šçŸ¥", "å…¬å‘Š", "ç®¡ç†åŠæ³•", "è§„å®š", "ä¸»ä»¶", "æ­£æ–‡"]
    if any(k in filename for k in main_keywords):
        return "main"
    return "attachment"

# ========== ğŸ†• æ ¸å¿ƒï¼šä½¿ç”¨ MarkdownElementNodeParser å¤„ç†æ–‡æ¡£ ==========

async def process_document_with_element_parser(
    file_path: str,
    filename: str,
    group_id: str,
    source_package: str
) -> dict:
    """
    ä½¿ç”¨ MarkdownElementNodeParser å¤„ç†æ–‡æ¡£
    åˆ†åˆ«å¤„ç†æ–‡æœ¬èŠ‚ç‚¹å’Œè¡¨æ ¼å¯¹è±¡
    """
    print(f"ğŸ“„ Processing: {filename}")

    # 1. ä½¿ç”¨ LlamaParse è§£ææ–‡æ¡£
    parser = LlamaParse(
        api_key=LLAMA_CLOUD_API_KEY,
        result_type="markdown",
        premium_mode=True,
        verbose=True,
        parsing_instruction="""
è¿™æ˜¯ä¸€ä¸ªç”µä¿¡è¿è¥å•†çš„æ¸ é“æ”¿ç­–æ–‡æ¡£ï¼Œè¯·æŒ‰ä»¥ä¸‹è¦æ±‚è§£æï¼š

ã€è¡¨æ ¼å¤„ç† - æœ€é«˜ä¼˜å…ˆçº§ã€‘
1. **å¿…é¡»ä¿ç•™æ‰€æœ‰è¡¨æ ¼çš„å®Œæ•´ç»“æ„**ï¼ŒåŒ…æ‹¬åµŒå¥—è¡¨æ ¼ã€åˆå¹¶å•å…ƒæ ¼
2. **è·¨é¡µè¡¨æ ¼å¿…é¡»åˆå¹¶**æˆä¸€ä¸ªå®Œæ•´çš„è¡¨æ ¼
3. è¡¨æ ¼è¾“å‡ºä¸º Markdown æ ¼å¼ï¼Œä½¿ç”¨æ ‡å‡†è¯­æ³•
4. **ä¸è¦é—æ¼ä»»ä½•æ•°å­—ã€é‡‘é¢ã€ç™¾åˆ†æ¯”**
5. ä¿ç•™è¡¨æ ¼æ ‡é¢˜å’Œè¯´æ˜æ–‡å­—

ã€æ–‡æœ¬å¤„ç†ã€‘
1. ä¿ç•™æ‰€æœ‰ä¸šåŠ¡åç§°ã€äº§å“åç§°ã€æ´»åŠ¨åç§°
2. ä¿ç•™å…³é”®æ¡æ¬¾ã€æ¡ä»¶è¯´æ˜ã€æ³¨æ„äº‹é¡¹
3. åˆ†çº§æ ‡é¢˜ç”¨ # ## ### ç­‰ Markdown è¯­æ³•æ ‡æ³¨

å…³é”®åŸåˆ™ï¼šå®å¯ä¿ç•™å¤šä½™ä¿¡æ¯ï¼Œä¹Ÿä¸è¦é—æ¼ä»»ä½•ä¸šåŠ¡è§„åˆ™å’Œæ•°å­—ï¼
        """.strip()
    )

    try:
        documents = await parser.aload_data(file_path)
        if not documents:
            print(f"âš ï¸ Warning: No text found in {filename}")
            return {"success": False, "error": "No documents parsed"}

        markdown_text = documents[0].text
        doc_type = guess_doc_type(filename)

        # ğŸ†• 2. æ£€æŸ¥æ˜¯å¦å¯ç”¨ MarkdownElementNodeParser
        if HAS_LLAMAINDEX:
            print("  âœ¨ Using MarkdownElementNodeParser (table extraction mode)")
            return await _process_with_element_parser(
                markdown_text, filename, group_id, source_package, doc_type
            )
        else:
            print("  âš ï¸ Using fallback mode (optimized for tables)")
            return await _process_with_fallback(
                markdown_text, filename, group_id, source_package, doc_type
            )

    except Exception as e:
        print(f"âŒ Error processing {filename}: {e}")
        traceback.print_exc()
        return {"success": False, "error": str(e)}


async def _process_with_element_parser(
    markdown_text: str,
    filename: str,
    group_id: str,
    source_package: str,
    doc_type: str
) -> dict:
    """ä½¿ç”¨ MarkdownElementNodeParser å¤„ç†ï¼ˆæ¨èæ¨¡å¼ï¼‰"""
    try:
        # ä½¿ç”¨ MarkdownElementNodeParser è§£æ
        node_parser = MarkdownElementNodeParser(
            num_workers=4,  # å¹¶å‘å¤„ç†
        )

        # åˆ›å»º LlamaIndex Document å¯¹è±¡
        llama_doc = Document(text=markdown_text, metadata={"filename": filename})

        # è·å–èŠ‚ç‚¹å’Œå¯¹è±¡
        nodes = node_parser.get_nodes_from_documents([llama_doc])
        base_nodes, objects = node_parser.get_nodes_and_objects(nodes)

        print(f"  ğŸ“Š Extracted {len(base_nodes)} text nodes")
        print(f"  ğŸ“‹ Extracted {len(objects)} table objects")

        total_stored = 0

        # ğŸ“Œ å­˜å‚¨æ–‡æœ¬èŠ‚ç‚¹
        from qdrant_client.models import PointStruct
        points_to_upload = []

        for i, node in enumerate(base_nodes):
            if node.text.strip():
                points_to_upload.append(
                    PointStruct(
                        id=str(uuid.uuid4()),
                        vector={},  # Qdrant ä¼šè‡ªåŠ¨ç”Ÿæˆå‘é‡
                        payload={
                            "document": node.text,
                            "group_id": group_id,
                            "filename": filename,
                            "doc_type": doc_type,
                            "chunk_type": "text",
                            "node_index": i,
                            "source_package": source_package
                        }
                    )
                )

        # ğŸ“Œ å­˜å‚¨è¡¨æ ¼å¯¹è±¡ï¼ˆå®Œæ•´è¡¨æ ¼ï¼Œä¸è¢«åˆ‡æ–­ï¼ï¼‰
        for i, obj in enumerate(objects):
            if obj.text.strip():
                points_to_upload.append(
                    PointStruct(
                        id=str(uuid.uuid4()),
                        vector={},  # Qdrant ä¼šè‡ªåŠ¨ç”Ÿæˆå‘é‡
                        payload={
                            "document": obj.text,
                            "group_id": group_id,
                            "filename": filename,
                            "doc_type": doc_type,
                            "chunk_type": "table",
                            "table_index": i,
                            "source_package": source_package,
                            "is_table": True
                        }
                    )
                )

        # æ‰¹é‡ä¸Šä¼ 
        if points_to_upload:
            client.upsert(
                collection_name=COLLECTION_NAME,
                points=points_to_upload
            )
            total_stored = len(points_to_upload)

        print(f"  âœ… Stored {total_stored} chunks (text + tables)")

        return {
            "success": True,
            "text_nodes": len(base_nodes),
            "table_objects": len(objects),
            "total_chunks": total_stored,
            "mode": "element_parser"
        }

    except Exception as e:
        print(f"âŒ Element Parser failed, falling back: {e}")
        traceback.print_exc()
        return await _process_with_fallback(
            markdown_text, filename, group_id, source_package, doc_type
        )


async def _process_with_fallback(
    markdown_text: str,
    filename: str,
    group_id: str,
    source_package: str,
    doc_type: str
) -> dict:
    """å›é€€æ¨¡å¼ï¼šä½¿ç”¨å¤§ chunk_size ä¿ç•™è¡¨æ ¼å®Œæ•´æ€§"""
    from langchain_text_splitters import RecursiveCharacterTextSplitter

    print("  ğŸ”„ Using fallback mode (large chunk size)")

    # ä½¿ç”¨æ›´å¤§çš„ chunk_size å‡å°‘åˆ‡æ–­è¡¨æ ¼çš„æ¦‚ç‡
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=4000,  # å¢å¤§åˆ°4000
        chunk_overlap=800,  # å¢å¤§ overlap
        separators=[
            "\n\n##",
            "\n\n###",
            "\n\n",
            "\n| ",  # å°è¯•åœ¨è¡¨æ ¼è¡Œå‰åˆ‡åˆ†
            "\n",
            "ã€‚",
            " ",
            ""
        ],
    )

    chunks = splitter.split_text(markdown_text)
    print(f"  ğŸ“Š Split into {len(chunks)} chunks")

    # ğŸ†• ä½¿ç”¨æ‰¹é‡ä¸Šä¼ 
    from qdrant_client.models import PointStruct
    points_to_upload = []

    for i, chunk in enumerate(chunks):
        if chunk.strip():
            # æ£€æµ‹æ˜¯å¦åŒ…å«è¡¨æ ¼
            is_table = "|" in chunk and ("|---" in chunk or "| ===" in chunk)

            points_to_upload.append(
                PointStruct(
                    id=str(uuid.uuid4()),
                    vector={},
                    payload={
                        "document": chunk,
                        "group_id": group_id,
                        "filename": filename,
                        "doc_type": doc_type,
                        "chunk_type": "table" if is_table else "text",
                        "chunk_index": i,
                        "source_package": source_package,
                        "is_table": is_table
                    }
                )
            )

    # æ‰¹é‡ä¸Šä¼ 
    if points_to_upload:
        client.upsert(
            collection_name=COLLECTION_NAME,
            points=points_to_upload
        )
        total_stored = len(points_to_upload)
    else:
        total_stored = 0

    print(f"  âœ… Stored {total_stored} chunks (fallback mode)")

    return {
        "success": True,
        "text_nodes": len([c for c in chunks if "|" not in c]),
        "table_objects": len([c for c in chunks if "|" in c]),
        "total_chunks": total_stored,
        "mode": "fallback"
    }

# ========== æ ¸å¿ƒä¸šåŠ¡ç«¯ç‚¹ ==========

@app.post("/ingest")
async def ingest_package(file: UploadFile = File(...), package_id: str = Form(None)):
    """
    æ–‡æ¡£å…¥åº“æ¥å£ - ğŸ†• ä½¿ç”¨ MarkdownElementNodeParser
    """
    if not LLAMA_CLOUD_API_KEY:
         raise HTTPException(status_code=500, detail="LLAMA_CLOUD_API_KEY not set.")

    group_id = package_id if package_id else str(uuid.uuid4())
    base_tmp_dir = f"/tmp/ingest_{group_id}"
    os.makedirs(base_tmp_dir, exist_ok=True)
    upload_path = f"{base_tmp_dir}/{file.filename}"

    try:
        content = await file.read()
        with open(upload_path, "wb") as f:
            f.write(content)

        files_to_process = []
        if file.filename.lower().endswith(".zip"):
            print(f"ğŸ“¦ Detected ZIP package: {file.filename}")
            extract_dir = f"{base_tmp_dir}/extracted"
            extract_zip(upload_path, extract_dir)
            for root, dirs, files in os.walk(extract_dir):
                for fname in files:
                    if fname.startswith(".") or "__MACOSX" in root: continue
                    files_to_process.append(os.path.join(root, fname))
        else:
            files_to_process.append(upload_path)

        # ğŸ†• ç»Ÿè®¡ä¿¡æ¯
        total_text_nodes = 0
        total_table_objects = 0
        processed_files = []

        # ğŸ†• ä½¿ç”¨æ–°çš„ Element Parser å¤„ç†æ¯ä¸ªæ–‡ä»¶
        for file_path in files_to_process:
            fname = os.path.basename(file_path)
            result = await process_document_with_element_parser(
                file_path=file_path,
                filename=fname,
                group_id=group_id,
                source_package=file.filename
            )

            if result["success"]:
                total_text_nodes += result.get("text_nodes", 0)
                total_table_objects += result.get("table_objects", 0)
                processed_files.append({
                    "filename": fname,
                    "status": "success",
                    "text_nodes": result.get("text_nodes", 0),
                    "table_objects": result.get("table_objects", 0)
                })
            else:
                processed_files.append({
                    "filename": fname,
                    "status": "failed",
                    "error": result.get("error", "Unknown error")
                })

        total_chunks = total_text_nodes + total_table_objects

        if total_chunks == 0:
            return {
                "status": "error",
                "msg": "No documents parsed successfully.",
                "processed_files": processed_files
            }

        return {
            "status": "success",
            "group_id": group_id,
            "total_text_nodes": total_text_nodes,
            "total_table_objects": total_table_objects,
            "total_chunks": total_chunks,
            "processed_files": processed_files
        }

    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        if os.path.exists(base_tmp_dir):
            shutil.rmtree(base_tmp_dir)

@app.post("/delete")
async def delete_package(target_id: str = Form(..., description="å¡«å…¥ group_id æˆ– file_id")):
    """åˆ é™¤æ–‡æ¡£ - ğŸ†• åŒæ—¶åˆ é™¤æ–‡æœ¬å’Œè¡¨æ ¼"""
    try:
        # åˆ é™¤ä¸»é›†åˆ
        if client.collection_exists(COLLECTION_NAME):
            client.delete(
                collection_name=COLLECTION_NAME,
                points_selector=models.FilterSelector(
                    filter=models.Filter(
                        must=[models.FieldCondition(key="group_id", match=models.MatchValue(value=target_id))]
                    )
                )
            )

        # ğŸ†• åˆ é™¤è¡¨æ ¼é›†åˆ
        if client.collection_exists(TABLES_COLLECTION_NAME):
            client.delete(
                collection_name=TABLES_COLLECTION_NAME,
                points_selector=models.FilterSelector(
                    filter=models.Filter(
                        must=[models.FieldCondition(key="group_id", match=models.MatchValue(value=target_id))]
                    )
                )
            )

        return {"status": "deleted", "target_id": target_id}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/reset")
async def reset_database():
    """
    ä¸€é”®é‡ç½®ï¼šåŒæ—¶æ¸…ç©º Qdrantï¼ˆæ–‡æœ¬+è¡¨æ ¼ï¼‰å’Œ Redis
    """
    report = []

    # 1. æ¸…ç©ºä¸»é›†åˆ
    try:
        client.delete_collection(COLLECTION_NAME)
        report.append("Qdrant text collection deleted")
    except Exception as e:
        report.append(f"Qdrant text skipped ({str(e)})")

    # ğŸ†• 2. æ¸…ç©ºè¡¨æ ¼é›†åˆ
    try:
        client.delete_collection(TABLES_COLLECTION_NAME)
        report.append("Qdrant tables collection deleted")
    except Exception as e:
        report.append(f"Qdrant tables skipped ({str(e)})")

    # 3. æ¸…ç©º Redis
    try:
        r = redis.Redis(
            host=REDIS_HOST,
            port=REDIS_PORT,
            password=REDIS_PASSWORD,
            decode_responses=True,
            socket_timeout=3
        )
        r.flushdb()
        report.append("Redis memory flushed")
    except Exception as e:
        print(f"âŒ Redis Reset Failed: {e}")
        report.append(f"Redis failed: {str(e)}")

    return {"status": "success", "details": " | ".join(report)}

@app.post("/search")
async def search_docs(query: str = Form(...), limit: int = 5):
    """
    ğŸ†• æœç´¢æ¥å£ - åŒæ—¶æœç´¢æ–‡æœ¬å’Œè¡¨æ ¼
    ä½¿ç”¨ query_points æ›¿ä»£å·²å¼ƒç”¨çš„ query æ–¹æ³•
    """
    try:
        all_results = []

        # ğŸ†• ä½¿ç”¨ä¸­æ–‡ embedding æ¨¡å‹
        from qdrant_client.models import Document, QueryType

        # 1. æœç´¢æ–‡æœ¬é›†åˆ
        if client.collection_exists(COLLECTION_NAME):
            print(f"ğŸ” Searching text collection for: {query}")
            text_results = client.query_points(
                collection_name=COLLECTION_NAME,
                query=Document(text=query, model="BAAI/bge-small-zh-v1.5"),  # ğŸ†• ä¸­æ–‡æ¨¡å‹
                limit=200,
                with_payload=True,
            )

            for res in text_results.points:
                # ğŸ†• ä» payload ä¸­æå–æ•°æ®
                all_results.append({
                    "id": str(res.id),
                    "text": res.payload.get("document", ""),
                    "meta": res.payload,
                    "source": "text",
                    "score": res.score  # ğŸ†• ç›´æ¥ä½¿ç”¨è¿”å›çš„ score
                })

        # 2. ğŸ†• æœç´¢è¡¨æ ¼é›†åˆï¼ˆé‡ç‚¹ï¼ï¼‰
        if client.collection_exists(TABLES_COLLECTION_NAME):
            print(f"ğŸ“‹ Searching tables collection for: {query}")
            table_results = client.query_points(
                collection_name=TABLES_COLLECTION_NAME,
                query=Document(text=query, model="BAAI/bge-small-zh-v1.5"),  # ğŸ†• ä¸­æ–‡æ¨¡å‹
                limit=100,
                with_payload=True,
            )

            for res in table_results.points:
                # ğŸ†• ä» payload ä¸­æå–æ•°æ®
                all_results.append({
                    "id": str(res.id),
                    "text": res.payload.get("document", ""),
                    "meta": res.payload,
                    "source": "table",
                    "score": res.score
                })

        if not all_results:
            return []

        print(f"  ğŸ“Š Found {len(all_results)} results (text + tables)")

        # 3. é‡æ’åºï¼ˆFlashRankï¼‰- ä»ç„¶æœ‰ç”¨ï¼Œå¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–ç»“æœ
        passages = [
            {"id": r["id"], "text": r["text"], "meta": r["meta"]}
            for r in all_results
        ]

        rerank_request = RerankRequest(query=query, passages=passages)
        ranked_results = reranker.rerank(rerank_request)

        top_results = ranked_results[:limit]

        # 4. ğŸ†• åœ¨ç»“æœä¸­æ ‡æ³¨æ¥æº
        return [
            {
                "content": res["text"],
                "score": float(res["score"]),
                "metadata": res["meta"],
                "content_type": "table" if res["meta"].get("is_table") else "text"
            }
            for res in top_results
        ]

    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

# ========== ğŸ†• Agentic RAG å¢å¼ºç«¯ç‚¹ ==========

@app.post("/analyze_query")
async def analyze_query(request: QueryAnalysisRequest):
    """åˆ†ææŸ¥è¯¢å¤æ‚åº¦ï¼Œè¿”å›æ‰§è¡Œè®¡åˆ’"""
    query = request.query.lower()

    analysis = {
        "query_type": "simple",
        "sub_queries": [],
        "required_tools": ["search"],
        "reasoning": "ç›´æ¥æ£€ç´¢",
        "suggested_approach": "single_step"
    }

    # æ£€æµ‹å…³é”®è¯
    comparison_keywords = ["å¯¹æ¯”", "å·®å¼‚", "å˜åŒ–", "vs", "åŒºåˆ«"]
    aggregation_keywords = ["æ€»è®¡", "ç»Ÿè®¡", "æ±‡æ€»", "å¹³å‡", "æ±‚å’Œ"]
    multi_year_keywords = ["2023", "2024", "2022", "2025", "å†å¹´", "é€å¹´"]
    table_keywords = ["è¡¨æ ¼", "excel", "é™„ä»¶", "sheet", "æ˜ç»†"]
    calculation_keywords = ["è®¡ç®—", "æ¿€åŠ±", "ææˆ", "é‡‘é¢", "è´¹ç”¨", "åˆè®¡"]

    has_comparison = any(kw in query for kw in comparison_keywords)
    has_aggregation = any(kw in query for kw in aggregation_keywords)
    has_multi_year = any(kw in query for kw in multi_year_keywords)
    has_table = any(kw in query for kw in table_keywords)
    has_calculation = any(kw in query for kw in calculation_keywords)

    # åˆ†ç±»é€»è¾‘
    if has_comparison and has_multi_year:
        analysis["query_type"] = "complex"
        analysis["required_tools"] = ["search", "compare"]
        analysis["suggested_approach"] = "parallel"
        analysis["reasoning"] = "æ£€æµ‹åˆ°è·¨å¹´åº¦å¯¹æ¯”æŸ¥è¯¢ï¼Œéœ€è¦åˆ†åˆ«æ£€ç´¢å„å¹´åº¦æ–‡æ¡£"

        years_found = []
        for year in ["2022", "2023", "2024", "2025"]:
            if year in query:
                years_found.append(year)

        if years_found:
            base_query = request.query
            for yr in years_found:
                base_query = base_query.replace(yr, "").replace("å†å¹´", "").replace("é€å¹´", "")

            analysis["sub_queries"] = [
                f"{yr}å¹´{base_query.strip()}".replace("  ", " ")
                for yr in years_found
            ]

    elif has_table:
        analysis["query_type"] = "table"
        analysis["required_tools"] = ["search", "extract_table"]
        analysis["reasoning"] = "æ£€æµ‹åˆ°è¡¨æ ¼æ•°æ®æŸ¥è¯¢ï¼Œä¼šä¼˜å…ˆä»è¡¨æ ¼é›†åˆæ£€ç´¢"

    elif has_aggregation or (has_calculation and "ã€" in query):
        analysis["query_type"] = "aggregation"
        analysis["required_tools"] = ["search", "calculate"]
        analysis["suggested_approach"] = "multi_step"
        analysis["reasoning"] = "æ£€æµ‹åˆ°æ•°æ®èšåˆæˆ–å¤æ‚è®¡ç®—éœ€æ±‚ï¼Œå»ºè®®åˆ†æ­¥æ£€ç´¢"

        if "ã€" in request.query:
            sub_questions = [q.strip() for q in request.query.split("ã€") if q.strip()]
            analysis["sub_queries"] = sub_questions

    else:
        analysis["reasoning"] = "ç®€å•æŸ¥è¯¢ï¼Œå°†åŒæ—¶æœç´¢æ–‡æœ¬å’Œè¡¨æ ¼"

    return analysis

@app.post("/extract_tables")
async def extract_tables(request: ExtractTableRequest):
    """
    ğŸ†• ä»è¡¨æ ¼é›†åˆä¸­æå–è¡¨æ ¼æ•°æ®
    """
    doc_id = request.document_id

    try:
        if not client.collection_exists(TABLES_COLLECTION_NAME):
            return {
                "document_id": doc_id,
                "table_count": 0,
                "tables": [],
                "error": "Tables collection not found"
            }

        # æœç´¢è¡¨æ ¼é›†åˆ
        search_result = client.query(
            collection_name=TABLES_COLLECTION_NAME,
            query_text=doc_id,
            limit=100
        )

        if not search_result:
            return {
                "document_id": doc_id,
                "table_count": 0,
                "tables": [],
                "message": "No tables found for this document"
            }

        tables = []
        for res in search_result:
            tables.append({
                "content": res.document,
                "source": res.metadata.get("filename", "unknown"),
                "chunk_id": str(res.id),
                "table_index": res.metadata.get("table_index", 0),
                "row_count": res.document.count("\n") + 1
            })

        return {
            "document_id": doc_id,
            "total_chunks": len(search_result),
            "table_count": len(tables),
            "tables": tables[:10]
        }

    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/compare_documents")
async def compare_documents(request: CompareDocumentsRequest):
    """ğŸ†• è·¨æ–‡æ¡£å¯¹æ¯” - åŒæ—¶æœç´¢æ–‡æœ¬å’Œè¡¨æ ¼"""
    doc_ids = request.doc_ids
    results = {}

    try:
        for doc_id in doc_ids:
            # æœç´¢ä¸»é›†åˆ
            text_results = []
            if client.collection_exists(COLLECTION_NAME):
                text_search = client.query(
                    collection_name=COLLECTION_NAME,
                    query_text=doc_id,
                    limit=30
                )
                text_results = [res.document for res in text_search[:3]]

            # æœç´¢è¡¨æ ¼é›†åˆ
            table_results = []
            if client.collection_exists(TABLES_COLLECTION_NAME):
                table_search = client.query(
                    collection_name=TABLES_COLLECTION_NAME,
                    query_text=doc_id,
                    limit=30
                )
                table_results = [res.document for res in table_search[:3]]

            results[doc_id] = {
                "text_chunks": len(text_results),
                "table_chunks": len(table_results),
                "text_samples": text_results,
                "table_samples": table_results
            }

        return {
            "comparison_result": results,
            "summary": f"å¯¹æ¯”äº† {len(doc_ids)} ä¸ªæ–‡æ¡£"
        }

    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

# ========== ğŸ†• ç»Ÿè®¡ä¿¡æ¯ç«¯ç‚¹ ==========

@app.get("/stats")
async def get_stats():
    """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
    stats = {
        "collections": {}
    }

    # ä¸»é›†åˆç»Ÿè®¡
    if client.collection_exists(COLLECTION_NAME):
        collection_info = client.get_collection(COLLECTION_NAME)
        stats["collections"]["text"] = {
            "name": COLLECTION_NAME,
            "points_count": collection_info.points_count,
            "status": "active"
        }
    else:
        stats["collections"]["text"] = {"status": "not_created"}

    # è¡¨æ ¼é›†åˆç»Ÿè®¡
    if client.collection_exists(TABLES_COLLECTION_NAME):
        collection_info = client.get_collection(TABLES_COLLECTION_NAME)
        stats["collections"]["tables"] = {
            "name": TABLES_COLLECTION_NAME,
            "points_count": collection_info.points_count,
            "status": "active"
        }
    else:
        stats["collections"]["tables"] = {"status": "not_created"}

    return stats

# ========== ç«¯ç‚¹æ€»ç»“ ==========
# /ingest       - ğŸ†• æ–‡æ¡£å…¥åº“ï¼ˆä½¿ç”¨ MarkdownElementNodeParserï¼‰
# /search       - ğŸ†• æœç´¢ï¼ˆåŒæ—¶æœç´¢æ–‡æœ¬å’Œè¡¨æ ¼ï¼‰
# /delete       - åˆ é™¤æ–‡æ¡£ï¼ˆåŒæ—¶åˆ é™¤æ–‡æœ¬å’Œè¡¨æ ¼ï¼‰
# /reset        - é‡ç½®æ•°æ®åº“ï¼ˆæ–‡æœ¬+è¡¨æ ¼+Redisï¼‰
# /stats        - ğŸ†• ç»Ÿè®¡ä¿¡æ¯
# /analyze_query - åˆ†ææŸ¥è¯¢å¤æ‚åº¦
# /extract_tables - æå–è¡¨æ ¼æ•°æ®
# /compare_documents - è·¨æ–‡æ¡£å¯¹æ¯”
