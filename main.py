import os
import uuid
import shutil
import zipfile
import traceback
from pathlib import Path
from typing import List

from fastapi import FastAPI, UploadFile, Form, HTTPException, File
from fastapi.middleware.cors import CORSMiddleware
from llama_parse import LlamaParse
from langchain_text_splitters import RecursiveCharacterTextSplitter
from qdrant_client import QdrantClient, models
from flashrank import Ranker, RerankRequest

# --- 1. ç¯å¢ƒå˜é‡è¯»å– ---
LLAMA_CLOUD_API_KEY = os.getenv("LLAMA_CLOUD_API_KEY")
QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")

# é›†åˆåç§° (ä¿æŒä¸å˜)
COLLECTION_NAME = "telecom_collection_v2"

print(f"DEBUG CONFIG: URL={QDRANT_URL}")

# --- 2. åˆå§‹åŒ– Re-ranker ---
print("â³ Initializing FlashRank Reranker...")
reranker = Ranker(model_name="ms-marco-MiniLM-L-12-v2", cache_dir="/tmp/flashrank_cache")
print("âœ… Reranker initialized!")

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

if not QDRANT_URL:
    raise ValueError("âŒ Fatal Error: QDRANT_URL is missing!")

# åˆå§‹åŒ– Qdrant
client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY, prefer_grpc=False)

@app.on_event("startup")
def startup_event():
    print(f"ğŸš€ Connecting to Qdrant at: {QDRANT_URL} ...")
    try:
        collections = client.get_collections()
        print(f"âœ… Connected! Found {len(collections.collections)} collections.")
    except Exception as e:
        print(f"âŒ Connection Failed! Error: {e}")

@app.get("/")
def health_check():
    return {"status": "ok", "service": "Telecom Complex Ingest API"}

# --- è¾…åŠ©å‡½æ•°ï¼šè§£å‹ ZIP ---
def extract_zip(zip_path: str, extract_to: str):
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_to)

# --- è¾…åŠ©å‡½æ•°ï¼šåˆ¤æ–­æ–‡ä»¶æ˜¯ä¸»ä»¶è¿˜æ˜¯é™„ä»¶ ---
def guess_doc_type(filename: str) -> str:
    # ç®€å•çš„å¯å‘å¼è§„åˆ™ï¼Œå¯æ ¹æ®ç”µä¿¡ä¸šåŠ¡ä¹ æƒ¯ä¿®æ”¹
    main_keywords = ["é€šçŸ¥", "å…¬å‘Š", "ç®¡ç†åŠæ³•", "è§„å®š", "ä¸»ä»¶", "æ­£æ–‡"]
    if any(k in filename for k in main_keywords):
        return "main"
    return "attachment"

@app.post("/ingest")
async def ingest_package(file: UploadFile = File(...), package_id: str = Form(None)):
    """
    é«˜çº§å…¥åº“æ¥å£ï¼š
    æ”¯æŒä¸Šä¼  .zip å‹ç¼©åŒ…ï¼ˆåŒ…å«ä¸»ä»¶+é™„ä»¶ï¼‰æˆ– å•ä¸ªæ–‡ä»¶ã€‚
    """
    if not LLAMA_CLOUD_API_KEY:
         raise HTTPException(status_code=500, detail="LLAMA_CLOUD_API_KEY not set.")

    # å¦‚æœæ²¡ä¼  IDï¼Œç”Ÿæˆä¸€ä¸ªæ–°çš„ Group ID (æ¡ˆå·ID)
    group_id = package_id if package_id else str(uuid.uuid4())
    
    # ä¸´æ—¶ç›®å½•
    base_tmp_dir = f"/tmp/ingest_{group_id}"
    os.makedirs(base_tmp_dir, exist_ok=True)
    
    upload_path = f"{base_tmp_dir}/{file.filename}"
    
    try:
        # 1. ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        content = await file.read()
        with open(upload_path, "wb") as f:
            f.write(content)
        
        files_to_process = []

        # 2. åˆ¤æ–­æ˜¯å¦ä¸º ZIP
        if file.filename.lower().endswith(".zip"):
            print(f"ğŸ“¦ Detected ZIP package: {file.filename}, extracting...")
            extract_dir = f"{base_tmp_dir}/extracted"
            extract_zip(upload_path, extract_dir)
            
            # éå†è§£å‹åçš„æ‰€æœ‰æ–‡ä»¶
            for root, dirs, files in os.walk(extract_dir):
                for fname in files:
                    if fname.startswith(".") or "__MACOSX" in root: continue # è·³è¿‡ç³»ç»Ÿéšè—æ–‡ä»¶
                    files_to_process.append(os.path.join(root, fname))
        else:
            # å•æ–‡ä»¶
            files_to_process.append(upload_path)

        print(f"task: Processing {len(files_to_process)} files in Group: {group_id}")

        # 3. åˆå§‹åŒ– LlamaParse (å¼€å¯é«˜çº§æ¨¡å¼ä»¥å¤„ç†å¤æ‚è¡¨æ ¼)
        parser = LlamaParse(
            api_key=LLAMA_CLOUD_API_KEY,
            result_type="markdown",
            premium_mode=True,  # âš ï¸ å¼€å¯é«˜çº§æ¨¡å¼ï¼Œè§£æè¡¨æ ¼æ›´å‡† (ä¼šæ¶ˆè€— Credit)
            verbose=True,
            language="zh"       # å¼ºåˆ¶ä¸­æ–‡è¯†åˆ«
        )

        total_chunks = 0
        all_points = [] # æš‚æ—¶å­˜æ”¾æ‰€æœ‰åˆ‡ç‰‡ï¼Œæœ€åä¸€èµ·å…¥åº“

        # 4. å¾ªç¯å¤„ç†æ¯ä¸ªæ–‡ä»¶
        for file_path in files_to_process:
            fname = os.path.basename(file_path)
            doc_type = guess_doc_type(fname) # è¯†åˆ«æ˜¯ä¸»ä»¶è¿˜æ˜¯é™„ä»¶
            
            print(f"ğŸ“„ Parsing ({doc_type}): {fname} ...")
            
            # LlamaParse è§£æ
            documents = await parser.aload_data(file_path)
            if not documents:
                print(f"âš ï¸ Warning: No text found in {fname}")
                continue
                
            markdown_text = documents[0].text
            
            # åˆ‡ç‰‡
            splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
            chunks = splitter.split_text(markdown_text)
            
            # å‡†å¤‡å…¥åº“æ•°æ® (æºå¸¦ Group ID å’Œ ç±»å‹)
            for i, chunk_text in enumerate(chunks):
                all_points.append({
                    "content": chunk_text,
                    "metadata": {
                        "group_id": group_id,     # æ ¸å¿ƒï¼šå…³è”ID
                        "filename": fname,
                        "doc_type": doc_type,     # main æˆ– attachment
                        "chunk_index": i,
                        "source_package": file.filename
                    }
                })
            
            total_chunks += len(chunks)

        # 5. æ‰¹é‡å…¥åº“
        if all_points:
            print(f"ğŸ’¾ Upserting {len(all_points)} total chunks to Qdrant...")
            
            # æå–æ–‡æœ¬åˆ—è¡¨ç”¨äºå‘é‡åŒ–
            texts = [p["content"] for p in all_points]
            metadatas = [p["metadata"] for p in all_points]
            ids = [str(uuid.uuid4()) for _ in all_points]

            client.add(
                collection_name=COLLECTION_NAME,
                documents=texts,
                metadata=metadatas,
                ids=ids
            )
        
        return {
            "status": "success", 
            "group_id": group_id, 
            "files_processed": len(files_to_process),
            "total_chunks": total_chunks
        }
        
    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        if os.path.exists(base_tmp_dir):
            shutil.rmtree(base_tmp_dir)

@app.post("/delete")
async def delete_package(group_id: str = Form(...)):
    """æŒ‰ Group ID åˆ é™¤æ•´å¥—æ–‡æ¡£"""
    try:
        client.delete(
            collection_name=COLLECTION_NAME,
            points_selector=models.FilterSelector(
                filter=models.Filter(
                    must=[
                        models.FieldCondition(
                            key="group_id",
                            match=models.MatchValue(value=group_id)
                        )
                    ]
                )
            )
        )
        return {"status": "deleted", "group_id": group_id}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/search")
async def search_docs(query: str = Form(...), limit: int = 5):
    """
    æ£€ç´¢æ¥å£ (ä¿æŒ FlashRank é‡æ’åºé€»è¾‘)
    """
    try:
        print(f"ğŸ” Searching for: {query}")
        
        search_result = client.query(
            collection_name=COLLECTION_NAME,
            query_text=query,
            limit=50 
        )
        
        if not search_result:
            return []

        passages = [
            {"id": str(res.id), "text": res.document, "meta": res.metadata}
            for res in search_result
        ]

        # Rerank
        rerank_request = RerankRequest(query=query, passages=passages)
        ranked_results = reranker.rerank(rerank_request)

        top_results = ranked_results[:limit]
        
        # è¿”å›ç»“æœ (ç°åœ¨åŒ…å«äº† filename, group_id ç­‰ä¸°å¯Œä¿¡æ¯)
        return [
            {
                "content": res["text"],
                "score": float(res["score"]),
                "metadata": res["meta"]
            } 
            for res in top_results
        ]

    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))