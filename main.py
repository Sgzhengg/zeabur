import os
import uuid
import shutil
import zipfile
import traceback
from pathlib import Path
from typing import List

# ğŸŸ¢ å¼•å…¥ Redis åº“
import redis

from fastapi import FastAPI, UploadFile, Form, HTTPException, File
from fastapi.middleware.cors import CORSMiddleware
from llama_parse import LlamaParse
from langchain_text_splitters import RecursiveCharacterTextSplitter
from qdrant_client import QdrantClient, models
from flashrank import Ranker, RerankRequest
from pydantic import BaseModel

# --- 1. ç¯å¢ƒå˜é‡è¯»å– ---
LLAMA_CLOUD_API_KEY = os.getenv("LLAMA_CLOUD_API_KEY")
QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")

# ğŸŸ¢ Redis é…ç½® (æ ¹æ®ä½ çš„æˆªå›¾ï¼Œé»˜è®¤ Host æ”¹ä¸º "redis")
REDIS_HOST = os.getenv("REDIS_HOST", "redis")
REDIS_PORT = int(os.getenv("REDIS_PORT", 6379))
REDIS_PASSWORD = os.getenv("REDIS_PASSWORD", None) # å¦‚æœæœ‰å¯†ç ï¼Œè¯·åœ¨ Zeabur å˜é‡é‡Œè®¾ç½®

COLLECTION_NAME = "telecom_collection_v2"

print(f"DEBUG CONFIG: QDRANT_URL={QDRANT_URL}, REDIS_HOST={REDIS_HOST}")

# --- 2. åˆå§‹åŒ– Re-ranker ---
print("â³ Initializing FlashRank Reranker...")
reranker = Ranker(model_name="ms-marco-MiniLM-L-12-v2", cache_dir="/tmp/flashrank_cache")
print("âœ… Reranker initialized!")

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

if not QDRANT_URL:
    raise ValueError("âŒ Fatal Error: QDRANT_URL is missing!")

# åˆå§‹åŒ– Qdrant
client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY, prefer_grpc=False)

@app.on_event("startup")
def startup_event():
    print(f"ğŸš€ Connecting to Qdrant at: {QDRANT_URL} ...")
    try:
        collections = client.get_collections()
        print(f"âœ… Connected to Qdrant! Found {len(collections.collections)} collections.")
    except Exception as e:
        print(f"âŒ Qdrant Connection Failed! Error: {e}")

@app.get("/")
def health_check():
    return {"status": "ok", "service": "Telecom Ingest API (With Agentic RAG Endpoints)"}

# ========== Pydantic æ•°æ®æ¨¡å‹ï¼ˆç”¨äºæ–°ç«¯ç‚¹ï¼‰ ==========

class QueryAnalysisRequest(BaseModel):
    query: str

class ExtractTableRequest(BaseModel):
    document_id: str

class CompareDocumentsRequest(BaseModel):
    doc_ids: List[str]

# ========== è¾…åŠ©å‡½æ•° ==========

def extract_zip(zip_path: str, extract_to: str):
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_to)

def guess_doc_type(filename: str) -> str:
    main_keywords = ["é€šçŸ¥", "å…¬å‘Š", "ç®¡ç†åŠæ³•", "è§„å®š", "ä¸»ä»¶", "æ­£æ–‡"]
    if any(k in filename for k in main_keywords):
        return "main"
    return "attachment"

# ========== æ ¸å¿ƒä¸šåŠ¡ç«¯ç‚¹ ==========

@app.post("/ingest")
async def ingest_package(file: UploadFile = File(...), package_id: str = Form(None)):
    """å…¥åº“æ¥å£"""
    if not LLAMA_CLOUD_API_KEY:
         raise HTTPException(status_code=500, detail="LLAMA_CLOUD_API_KEY not set.")

    group_id = package_id if package_id else str(uuid.uuid4())
    base_tmp_dir = f"/tmp/ingest_{group_id}"
    os.makedirs(base_tmp_dir, exist_ok=True)
    upload_path = f"{base_tmp_dir}/{file.filename}"

    try:
        content = await file.read()
        with open(upload_path, "wb") as f:
            f.write(content)

        files_to_process = []
        if file.filename.lower().endswith(".zip"):
            print(f"ğŸ“¦ Detected ZIP package: {file.filename}")
            extract_dir = f"{base_tmp_dir}/extracted"
            extract_zip(upload_path, extract_dir)
            for root, dirs, files in os.walk(extract_dir):
                for fname in files:
                    if fname.startswith(".") or "__MACOSX" in root: continue
                    files_to_process.append(os.path.join(root, fname))
        else:
            files_to_process.append(upload_path)

        parser = LlamaParse(
            api_key=LLAMA_CLOUD_API_KEY,
            result_type="markdown",
            premium_mode=True,
            verbose=True,
            parsing_instruction="è¿™æ˜¯ä¸€ä¸ªç”µä¿¡è¿è¥å•†çš„æ”¿ç­–æ–‡æ¡£ï¼ŒåŒ…å«å¤§é‡å¤æ‚çš„åµŒå¥—è¡¨æ ¼ã€‚è¯·å°½å¯èƒ½ä¿ç•™è¡¨æ ¼çš„ç»“æ„ï¼Œä¸è¦é—æ¼ä»»ä½•æ•°å­—ã€‚å¦‚æœè¡¨æ ¼è·¨é¡µï¼Œè¯·å°†å…¶åˆå¹¶ã€‚"
        )

        total_chunks = 0
        all_points = []

        for file_path in files_to_process:
            fname = os.path.basename(file_path)
            doc_type = guess_doc_type(fname)
            print(f"ğŸ“„ Parsing ({doc_type}): {fname}")

            try:
                documents = await parser.aload_data(file_path)
            except Exception as parse_error:
                print(f"âŒ Parse Error on {fname}: {parse_error}")
                continue

            if not documents:
                print(f"âš ï¸ Warning: No text found in {fname}")
                continue

            markdown_text = documents[0].text

            splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=500)
            chunks = splitter.split_text(markdown_text)

            for i, chunk_text in enumerate(chunks):
                all_points.append({
                    "content": chunk_text,
                    "metadata": {
                        "group_id": group_id,
                        "filename": fname,
                        "doc_type": doc_type,
                        "chunk_index": i,
                        "source_package": file.filename
                    }
                })
            total_chunks += len(chunks)

        if total_chunks == 0:
            return {"status": "error", "msg": "No documents parsed."}

        if all_points:
            print(f"ğŸ’¾ Upserting {len(all_points)} chunks...")
            texts = [p["content"] for p in all_points]
            metadatas = [p["metadata"] for p in all_points]
            ids = [str(uuid.uuid4()) for _ in all_points]

            client.add(
                collection_name=COLLECTION_NAME,
                documents=texts,
                metadata=metadatas,
                ids=ids
            )

        return {"status": "success", "group_id": group_id, "chunks": total_chunks}

    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        if os.path.exists(base_tmp_dir):
            shutil.rmtree(base_tmp_dir)

@app.post("/delete")
async def delete_package(target_id: str = Form(..., description="å¡«å…¥ group_id æˆ– file_id")):
    try:
        if not client.collection_exists(COLLECTION_NAME):
             return {"status": "skipped", "msg": "Collection does not exist."}

        client.delete(
            collection_name=COLLECTION_NAME,
            points_selector=models.FilterSelector(
                filter=models.Filter(
                    must=[models.FieldCondition(key="group_id", match=models.MatchValue(value=target_id))]
                )
            )
        )
        client.delete(
            collection_name=COLLECTION_NAME,
            points_selector=models.FilterSelector(
                filter=models.Filter(
                    must=[models.FieldCondition(key="file_id", match=models.MatchValue(value=target_id))]
                )
            )
        )
        return {"status": "deleted", "target_id": target_id}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/reset")
async def reset_database():
    """
    ä¸€é”®é‡ç½®ï¼šåŒæ—¶æ¸…ç©º Qdrant å’Œ Redis
    """
    report = []

    # 1. æ¸…ç©º Qdrant
    try:
        client.delete_collection(COLLECTION_NAME)
        report.append("Qdrant collection deleted")
    except Exception as e:
        # å¦‚æœé›†åˆæœ¬æ¥å°±ä¸å­˜åœ¨ï¼Œä¸ç®—é”™
        report.append(f"Qdrant skipped ({str(e)})")

    # 2. ğŸŸ¢ æ¸…ç©º Redis (è®°å¿†)
    try:
        # è¿æ¥åˆ° Redis
        r = redis.Redis(
            host=REDIS_HOST,
            port=REDIS_PORT,
            password=REDIS_PASSWORD,
            decode_responses=True,
            socket_timeout=3 # è®¾ç½®è¶…æ—¶é˜²æ­¢å¡æ­»
        )
        # æ‰§è¡Œæ¸…ç©ºæŒ‡ä»¤
        r.flushdb()
        report.append("Redis memory flushed")
    except Exception as e:
        print(f"âŒ Redis Reset Failed: {e}")
        report.append(f"Redis failed: {str(e)}")

    return {"status": "success", "details": " | ".join(report)}

@app.post("/search")
async def search_docs(query: str = Form(...), limit: int = 5):
    try:
        if not client.collection_exists(COLLECTION_NAME):
            return []

        print(f"ğŸ” Searching for: {query}")

        search_result = client.query(
            collection_name=COLLECTION_NAME,
            query_text=query,
            limit=300
        )

        if not search_result:
            return []

        passages = [
            {"id": str(res.id), "text": res.document, "meta": res.metadata}
            for res in search_result
        ]

        rerank_request = RerankRequest(query=query, passages=passages)
        ranked_results = reranker.rerank(rerank_request)

        top_results = ranked_results[:limit]

        return [
            {
                "content": res["text"],
                "score": float(res["score"]),
                "metadata": res["meta"]
            }
            for res in top_results
        ]

    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

# ========== ğŸ†• Agentic RAG å¢å¼ºç«¯ç‚¹ ==========

@app.post("/analyze_query")
async def analyze_query(request: QueryAnalysisRequest):
    """
    åˆ†ææŸ¥è¯¢å¤æ‚åº¦ï¼Œè¿”å›æ‰§è¡Œè®¡åˆ’
    å¸®åŠ© AI Agent å†³å®šæ£€ç´¢ç­–ç•¥
    """
    query = request.query.lower()

    # é»˜è®¤ç®€å•æŸ¥è¯¢
    analysis = {
        "query_type": "simple",              # simple | complex | table | aggregation
        "sub_queries": [],                    # åˆ†è§£åçš„å­æŸ¥è¯¢
        "required_tools": ["search"],         # éœ€è¦çš„å·¥å…·
        "reasoning": "ç›´æ¥æ£€ç´¢",              # æ¨ç†è¯´æ˜
        "suggested_approach": "single_step"   # single_step | multi_step | parallel
    }

    # æ£€æµ‹å…³é”®è¯
    comparison_keywords = ["å¯¹æ¯”", "å·®å¼‚", "å˜åŒ–", "vs", "åŒºåˆ«"]
    aggregation_keywords = ["æ€»è®¡", "ç»Ÿè®¡", "æ±‡æ€»", "å¹³å‡", "æ±‚å’Œ"]
    multi_year_keywords = ["2023", "2024", "2022", "2025", "å†å¹´", "é€å¹´"]
    table_keywords = ["è¡¨æ ¼", "excel", "é™„ä»¶", "sheet", "æ˜ç»†"]
    calculation_keywords = ["è®¡ç®—", "æ¿€åŠ±", "ææˆ", "é‡‘é¢", "è´¹ç”¨", "åˆè®¡"]

    has_comparison = any(kw in query for kw in comparison_keywords)
    has_aggregation = any(kw in query for kw in aggregation_keywords)
    has_multi_year = any(kw in query for kw in multi_year_keywords)
    has_table = any(kw in query for kw in table_keywords)
    has_calculation = any(kw in query for kw in calculation_keywords)

    # åˆ†ç±»é€»è¾‘
    if has_comparison and has_multi_year:
        # å¤æ‚è·¨å¹´åº¦å¯¹æ¯”æŸ¥è¯¢
        analysis["query_type"] = "complex"
        analysis["required_tools"] = ["search", "compare"]
        analysis["suggested_approach"] = "parallel"
        analysis["reasoning"] = "æ£€æµ‹åˆ°è·¨å¹´åº¦å¯¹æ¯”æŸ¥è¯¢ï¼Œéœ€è¦åˆ†åˆ«æ£€ç´¢å„å¹´åº¦æ–‡æ¡£"

        # æå–å¹´ä»½å¹¶åˆ†è§£æŸ¥è¯¢
        years_found = []
        for year in ["2022", "2023", "2024", "2025"]:
            if year in query:
                years_found.append(year)

        if years_found:
            # ç§»é™¤å¹´ä»½ï¼Œä¿ç•™æ ¸å¿ƒé—®é¢˜
            base_query = request.query
            for yr in years_found:
                base_query = base_query.replace(yr, "").replace("å†å¹´", "").replace("é€å¹´", "")

            # ç”Ÿæˆå­æŸ¥è¯¢
            analysis["sub_queries"] = [
                f"{yr}å¹´{base_query.strip()}".replace("  ", " ")
                for yr in years_found
            ]

    elif has_table:
        # è¡¨æ ¼æ•°æ®æå–
        analysis["query_type"] = "table"
        analysis["required_tools"] = ["search", "extract_table"]
        analysis["reasoning"] = "æ£€æµ‹åˆ°è¡¨æ ¼æ•°æ®æŸ¥è¯¢ï¼Œå»ºè®®ä¼˜å…ˆæå– Excel é™„ä»¶"

    elif has_aggregation or (has_calculation and "ã€" in query):
        # æ•°æ®èšåˆæˆ–å¤æ‚è®¡ç®—
        analysis["query_type"] = "aggregation"
        analysis["required_tools"] = ["search", "calculate"]
        analysis["suggested_approach"] = "multi_step"
        analysis["reasoning"] = "æ£€æµ‹åˆ°æ•°æ®èšåˆæˆ–å¤æ‚è®¡ç®—éœ€æ±‚ï¼Œå»ºè®®åˆ†æ­¥æ£€ç´¢"

        # å¦‚æœåŒ…å«å¤šä¸ªé—®é¢˜ï¼ˆé¡¿å·åˆ†éš”ï¼‰
        if "ã€" in request.query:
            sub_questions = [q.strip() for q in request.query.split("ã€") if q.strip()]
            analysis["sub_queries"] = sub_questions

    else:
        # ç®€å•æŸ¥è¯¢
        analysis["reasoning"] = "ç®€å•æŸ¥è¯¢ï¼Œå¯ç›´æ¥æ£€ç´¢"

    return analysis

@app.post("/extract_tables")
async def extract_tables(request: ExtractTableRequest):
    """
    ä»æ–‡æ¡£ä¸­æå–è¡¨æ ¼æ•°æ®
    è¯†åˆ« Markdown æ ¼å¼çš„è¡¨æ ¼å¹¶è¿”å›ç»“æ„åŒ–æ•°æ®
    """
    doc_id = request.document_id

    try:
        if not client.collection_exists(COLLECTION_NAME):
            return {
                "document_id": doc_id,
                "table_count": 0,
                "tables": [],
                "error": "Collection not found"
            }

        # æœç´¢è¯¥æ–‡æ¡£çš„æ‰€æœ‰ç‰‡æ®µ
        search_result = client.query(
            collection_name=COLLECTION_NAME,
            query_text=doc_id,  # ç”¨æ–‡æ¡£å/IDä½œä¸ºæŸ¥è¯¢
            limit=100
        )

        if not search_result:
            return {
                "document_id": doc_id,
                "table_count": 0,
                "tables": [],
                "message": "No content found for this document"
            }

        # è¿‡æ»¤å¹¶æå–è¡¨æ ¼å†…å®¹
        tables = []
        for res in search_result:
            content = res.document

            # ç®€å•æ£€æµ‹ Markdown è¡¨æ ¼ï¼šåŒ…å« | å’Œåˆ†éš”çº¿
            if "|" in content and ("|---" in content or "| ===" in content):
                tables.append({
                    "content": content,
                    "source": res.metadata.get("filename", "unknown"),
                    "chunk_id": str(res.id),
                    "doc_type": res.metadata.get("doc_type", "unknown"),
                    "row_count": content.count("\n") + 1  # ä¼°ç®—è¡Œæ•°
                })

        return {
            "document_id": doc_id,
            "total_chunks": len(search_result),
            "table_count": len(tables),
            "tables": tables[:10]  # æœ€å¤šè¿”å›10ä¸ªè¡¨æ ¼ï¼Œé¿å…è¿‡å¤§
        }

    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/compare_documents")
async def compare_documents(request: CompareDocumentsRequest):
    """
    è·¨æ–‡æ¡£å¯¹æ¯”
    æå–å¤šä¸ªæ–‡æ¡£çš„å…³é”®ä¿¡æ¯ï¼Œä¾¿äº Agent è¿›è¡Œå¯¹æ¯”åˆ†æ
    """
    doc_ids = request.doc_ids
    results = {}

    try:
        if not client.collection_exists(COLLECTION_NAME):
            return {
                "comparison_result": {},
                "error": "Collection not found"
            }

        for doc_id in doc_ids:
            # æœç´¢æ¯ä¸ªæ–‡æ¡£
            search_result = client.query(
                collection_name=COLLECTION_NAME,
                query_text=doc_id,
                limit=50
            )

            if not search_result:
                results[doc_id] = {
                    "found": False,
                    "message": "No content found"
                }
                continue

            # æå–å…³é”®ä¿¡æ¯
            # 1. æ–‡ä»¶å
            filenames = set(res.metadata.get("filename", "") for res in search_result)

            # 2. å…³é”®ç‰‡æ®µï¼ˆå–å‰3ä¸ªç›¸å…³åº¦æœ€é«˜çš„ï¼‰
            key_points = [res.document for res in search_result[:3]]

            # 3. æ–‡æ¡£ç±»å‹
            doc_types = set(res.metadata.get("doc_type", "") for res in search_result)

            results[doc_id] = {
                "found": True,
                "filenames": list(filenames),
                "doc_types": list(doc_types),
                "total_chunks": len(search_result),
                "key_points": key_points,
                "sample_metadata": search_result[0].metadata if search_result else {}
            }

        return {
            "comparison_result": results,
            "summary": {
                "documents_compared": len(doc_ids),
                "successful": sum(1 for r in results.values() if r.get("found", False)),
                "failed": sum(1 for r in results.values() if not r.get("found", False))
            }
        }

    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

# ========== ç«¯ç‚¹æ€»ç»“ ==========
# /ingest       - æ–‡æ¡£å…¥åº“ï¼ˆZIP/å•æ–‡ä»¶ï¼‰
# /search       - å‘é‡æœç´¢ + é‡æ’åº
# /delete       - åˆ é™¤æ–‡æ¡£
# /reset        - é‡ç½®æ•°æ®åº“ï¼ˆQdrant + Redisï¼‰
# /analyze_query - ğŸ†• åˆ†ææŸ¥è¯¢å¤æ‚åº¦
# /extract_tables - ğŸ†• æå–è¡¨æ ¼æ•°æ®
# /compare_documents - ğŸ†• è·¨æ–‡æ¡£å¯¹æ¯”
