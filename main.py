import os
import uuid
import shutil
import zipfile
import traceback
from pathlib import Path
from typing import List

from fastapi import FastAPI, UploadFile, Form, HTTPException, File
from fastapi.middleware.cors import CORSMiddleware
from llama_parse import LlamaParse
from langchain_text_splitters import RecursiveCharacterTextSplitter
from qdrant_client import QdrantClient, models
from flashrank import Ranker, RerankRequest

# --- 1. ç¯å¢ƒå˜é‡è¯»å– ---
LLAMA_CLOUD_API_KEY = os.getenv("LLAMA_CLOUD_API_KEY")
QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")

COLLECTION_NAME = "telecom_collection_v2"

print(f"DEBUG CONFIG: URL={QDRANT_URL}")

# --- 2. åˆå§‹åŒ– Re-ranker ---
print("â³ Initializing FlashRank Reranker...")
# ä¾ç„¶ä½¿ç”¨è¿™ä¸ªé€Ÿåº¦å¿«ä¸”æ•ˆæœå¥½çš„è½»é‡æ¨¡å‹
reranker = Ranker(model_name="ms-marco-MiniLM-L-12-v2", cache_dir="/tmp/flashrank_cache")
print("âœ… Reranker initialized!")

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

if not QDRANT_URL:
    raise ValueError("âŒ Fatal Error: QDRANT_URL is missing!")

client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY, prefer_grpc=False)

@app.on_event("startup")
def startup_event():
    print(f"ğŸš€ Connecting to Qdrant at: {QDRANT_URL} ...")
    try:
        collections = client.get_collections()
        print(f"âœ… Connected! Found {len(collections.collections)} collections.")
    except Exception as e:
        print(f"âŒ Connection Failed! Error: {e}")

@app.get("/")
def health_check():
    return {"status": "ok", "service": "Telecom Ingest API Optimized"}

# --- è¾…åŠ©å‡½æ•° ---
def extract_zip(zip_path: str, extract_to: str):
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_to)

def guess_doc_type(filename: str) -> str:
    main_keywords = ["é€šçŸ¥", "å…¬å‘Š", "ç®¡ç†åŠæ³•", "è§„å®š", "ä¸»ä»¶", "æ­£æ–‡"]
    if any(k in filename for k in main_keywords):
        return "main"
    return "attachment"

@app.post("/ingest")
async def ingest_package(file: UploadFile = File(...), package_id: str = Form(None)):
    """
    å…¥åº“æ¥å£ï¼šæ”¯æŒ ZIP åŒ…ï¼Œé’ˆå¯¹ç”µä¿¡æ–‡æ¡£ä¼˜åŒ–äº†è§£ææŒ‡ä»¤å’Œåˆ‡ç‰‡å¤§å°
    """
    if not LLAMA_CLOUD_API_KEY:
         raise HTTPException(status_code=500, detail="LLAMA_CLOUD_API_KEY not set.")

    group_id = package_id if package_id else str(uuid.uuid4())
    base_tmp_dir = f"/tmp/ingest_{group_id}"
    os.makedirs(base_tmp_dir, exist_ok=True)
    upload_path = f"{base_tmp_dir}/{file.filename}"
    
    try:
        content = await file.read()
        with open(upload_path, "wb") as f:
            f.write(content)
        
        files_to_process = []
        if file.filename.lower().endswith(".zip"):
            print(f"ğŸ“¦ Detected ZIP package: {file.filename}")
            extract_dir = f"{base_tmp_dir}/extracted"
            extract_zip(upload_path, extract_dir)
            for root, dirs, files in os.walk(extract_dir):
                for fname in files:
                    if fname.startswith(".") or "__MACOSX" in root: continue
                    files_to_process.append(os.path.join(root, fname))
        else:
            files_to_process.append(upload_path)

        # ğŸŸ¢ ä¼˜åŒ–ç‚¹ 1ï¼šç§»é™¤ language="zh" é˜²æ­¢æŠ¥é”™ï¼Œå¢åŠ è§£ææŒ‡ä»¤ä¼˜åŒ–è¡¨æ ¼
        parser = LlamaParse(
            api_key=LLAMA_CLOUD_API_KEY,
            result_type="markdown",
            premium_mode=True, 
            verbose=True,
            parsing_instruction="è¿™æ˜¯ä¸€ä¸ªç”µä¿¡è¿è¥å•†çš„æ”¿ç­–æ–‡æ¡£ï¼ŒåŒ…å«å¤§é‡å¤æ‚çš„åµŒå¥—è¡¨æ ¼ã€‚è¯·å°½å¯èƒ½ä¿ç•™è¡¨æ ¼çš„ç»“æ„ï¼Œä¸è¦é—æ¼ä»»ä½•æ•°å­—ã€‚å¦‚æœè¡¨æ ¼è·¨é¡µï¼Œè¯·å°†å…¶åˆå¹¶ã€‚"
        )

        total_chunks = 0
        all_points = [] 

        for file_path in files_to_process:
            fname = os.path.basename(file_path)
            doc_type = guess_doc_type(fname)
            print(f"ğŸ“„ Parsing ({doc_type}): {fname}")
            
            try:
                documents = await parser.aload_data(file_path)
            except Exception as parse_error:
                print(f"âŒ Parse Error on {fname}: {parse_error}")
                continue

            if not documents: 
                print(f"âš ï¸ Warning: No text found in {fname}")
                continue
                
            markdown_text = documents[0].text
            
            # ğŸŸ¢ ä¼˜åŒ–ç‚¹ 2ï¼šå¢å¤§ chunk_size åˆ° 2000ï¼Œoverlap åˆ° 500
            # è¿™æ ·èƒ½ä¿è¯ä¸Šä¸‹æ–‡è¿è´¯ï¼Œè§£å†³"ä¸çŸ¥é“æ˜¯å“ªä¸ªæœˆä»½"çš„é—®é¢˜
            splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=500)
            chunks = splitter.split_text(markdown_text)
            
            for i, chunk_text in enumerate(chunks):
                all_points.append({
                    "content": chunk_text,
                    "metadata": {
                        "group_id": group_id,
                        "filename": fname,
                        "doc_type": doc_type,
                        "chunk_index": i,
                        "source_package": file.filename
                    }
                })
            total_chunks += len(chunks)

        if total_chunks == 0:
            return {"status": "error", "msg": "No documents parsed."}

        if all_points:
            print(f"ğŸ’¾ Upserting {len(all_points)} chunks...")
            texts = [p["content"] for p in all_points]
            metadatas = [p["metadata"] for p in all_points]
            ids = [str(uuid.uuid4()) for _ in all_points]

            client.add(
                collection_name=COLLECTION_NAME,
                documents=texts,
                metadata=metadatas,
                ids=ids
            )
        
        return {"status": "success", "group_id": group_id, "chunks": total_chunks}
        
    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        if os.path.exists(base_tmp_dir):
            shutil.rmtree(base_tmp_dir)

@app.post("/delete")
async def delete_package(target_id: str = Form(..., description="å¡«å…¥ group_id æˆ– file_id")):
    try:
        if not client.collection_exists(COLLECTION_NAME):
             return {"status": "skipped", "msg": "Collection does not exist."}

        # åˆ é™¤ group_id åŒ¹é…çš„
        client.delete(
            collection_name=COLLECTION_NAME,
            points_selector=models.FilterSelector(
                filter=models.Filter(
                    must=[models.FieldCondition(key="group_id", match=models.MatchValue(value=target_id))]
                )
            )
        )
        # åˆ é™¤ file_id åŒ¹é…çš„ (å…¼å®¹æ—§æ•°æ®)
        client.delete(
            collection_name=COLLECTION_NAME,
            points_selector=models.FilterSelector(
                filter=models.Filter(
                    must=[models.FieldCondition(key="file_id", match=models.MatchValue(value=target_id))]
                )
            )
        )
        return {"status": "deleted", "target_id": target_id}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/reset")
async def reset_database():
    """ä»…åˆ é™¤é›†åˆï¼Œä¸é‡æ–°åˆ›å»ºï¼Œç”± ingest è‡ªåŠ¨å¤„ç†é‡å»º"""
    try:
        client.delete_collection(COLLECTION_NAME)
        return {"status": "success", "msg": "Collection deleted."}
    except Exception as e:
        return {"status": "success", "msg": "Collection already clear."}

@app.post("/search")
async def search_docs(query: str = Form(...), limit: int = 5):
    try:
        # é˜²æ­¢åˆš reset å®ŒæŠ¥é”™
        if not client.collection_exists(COLLECTION_NAME):
            return []

        print(f"ğŸ” Searching for: {query}")
        
        # ğŸŸ¢ ä¼˜åŒ–ç‚¹ 3ï¼šæ‰©å¤§åˆç­›èŒƒå›´åˆ° 100 æ¡ï¼Œå®æ»¥å‹¿ç¼º
        search_result = client.query(
            collection_name=COLLECTION_NAME,
            query_text=query,
            limit=100 
        )
        
        if not search_result:
            return []

        passages = [
            {"id": str(res.id), "text": res.document, "meta": res.metadata}
            for res in search_result
        ]

        print(f"âš–ï¸ Reranking {len(passages)} documents...")
        rerank_request = RerankRequest(query=query, passages=passages)
        ranked_results = reranker.rerank(rerank_request)

        top_results = ranked_results[:limit]
        
        # ğŸŸ¢ ä¼˜åŒ–ç‚¹ 4ï¼šå¼ºåˆ¶ float è½¬æ¢ï¼Œä¿®å¤ 500 æŠ¥é”™
        return [
            {
                "content": res["text"],
                "score": float(res["score"]),
                "metadata": res["meta"]
            } 
            for res in top_results
        ]

    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))